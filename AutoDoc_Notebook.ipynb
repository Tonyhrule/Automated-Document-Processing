{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d01fb0",
   "metadata": {},
   "source": [
    "# Automating Trustworthy Document Processing with Cleanlab and Unstructured\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In today's AI landscape, ensuring the reliability and accuracy of AI-generated responses is essential. This notebook demonstrates how to build trustworthy document processing systems by combining Cleanlab's Trustworthy Language Model (TLM) and Unstructured's document parsing capabilities.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) enhances large language model performance by grounding responses in retrieved information. However, RAG effectiveness depends heavily on the quality and accessibility of underlying data. Many valuable sources exist in complex formats like PDFs, spreadsheets, and images that require sophisticated preprocessing for RAG pipelines.\n",
    "\n",
    "This notebook addresses three key challenges:\n",
    "\n",
    "1. **Document Preprocessing**: Converting diverse formats into structured text while preserving semantic relationships\n",
    "2. **Hallucination Mitigation**: Reducing false information generation by grounding LLM responses in verified data\n",
    "3. **Trustworthiness Assessment**: Quantifying response reliability for appropriate human oversight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17058ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q cleanlab-tlm llama-index llama-index-embeddings-huggingface unstructured_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c1182",
   "metadata": {},
   "source": [
    "### API Key Configuration\n",
    "\n",
    "To use this notebook, you'll need to obtain API keys for both Cleanlab and Unstructured:\n",
    "- Cleanlab TLM API key: Available at [Cleanlab Studio](https://cleanlab.ai/)\n",
    "- Unstructured API key: Available at [Unstructured.io](https://unstructured.io/)\n",
    "\n",
    "Enter your API keys in the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CLEANLAB_API_KEY\"] = \"\"  # Replace with your API key\n",
    "os.environ[\"UNSTRUCTURED_API_KEY\"] = \"\"  # Replace with your API key\n",
    "os.environ[\"UNSTRUCTURED_API_URL\"] = \"https://api.unstructured.io/general/v0/general\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9254c67",
   "metadata": {},
   "source": [
    "## TLM Integration with LlamaIndex\n",
    "\n",
    "Cleanlab's Trustworthy Language Model (TLM) provides reliable responses with a trustworthiness score indicating confidence. This feature is valuable in RAG systems where response accuracy is critical.\n",
    "\n",
    "In this section, we'll integrate TLM with LlamaIndex by:\n",
    "\n",
    "1. Initializing a connection to Cleanlab's TLM\n",
    "2. Creating a custom wrapper for use with LlamaIndex\n",
    "3. Setting up an embedding model for semantic search\n",
    "4. Defining a function to create query engines\n",
    "\n",
    "The trustworthiness scores help identify potential hallucinations or low-confidence responses for more reliable applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio\n",
    "\n",
    "studio = Studio(os.environ[\"CLEANLAB_API_KEY\"])\n",
    "tlm = studio.TLM()\n",
    "\n",
    "import os\n",
    "from typing import Any\n",
    "import json\n",
    "from llama_index.core.base.llms.types import (\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core.llms.custom import CustomLLM\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "\n",
    "\n",
    "studio = Studio(os.environ[\"CLEANLAB_API_KEY\"])\n",
    "tlm = studio.TLM()\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\n",
    "class TLMWrapper(CustomLLM):\n",
    "    context_window: int = 16000\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"TLM\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        # Prompt tlm for a response and trustworthiness score\n",
    "        response = tlm.prompt(prompt)\n",
    "        output = json.dumps(response)\n",
    "        return CompletionResponse(text=output)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        # Prompt tlm for a response and trustworthiness score\n",
    "        response = tlm.prompt(prompt)\n",
    "        output = json.dumps(response)\n",
    "\n",
    "        # Stream the output\n",
    "        output_str = \"\"\n",
    "        for token in output:\n",
    "            output_str += token\n",
    "            yield CompletionResponse(text=output_str, delta=token)\n",
    "\n",
    "\n",
    "def tlm_query_engine(documents: list[str]):\n",
    "    return VectorStoreIndex(\n",
    "        [Document(text=document, metadata={\"source\": \"tlm\"}) for document in documents]\n",
    "    ).as_query_engine(llm=TLMWrapper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f790957",
   "metadata": {},
   "source": [
    "## Unstructured API Setup\n",
    "\n",
    "Unstructured is a document processing platform that extracts structured information from various formats including PDFs, Word documents, and HTML. It excels at handling complex layouts, tables, and structured elements challenging for traditional parsers.\n",
    "\n",
    "We'll set up the Unstructured API client to access these capabilities:\n",
    "\n",
    "1. **Document Partitioning**: Breaking documents into meaningful chunks\n",
    "2. **Table Extraction**: Accurately identifying and extracting tabular data\n",
    "3. **Layout Analysis**: Understanding document structure and element relationships\n",
    "4. **Multi-format Support**: Processing various document formats consistently\n",
    "\n",
    "These capabilities are essential for RAG systems leveraging diverse document sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unstructured_client\n",
    "from unstructured_client.models import operations, shared\n",
    "import requests\n",
    "\n",
    "\n",
    "client = unstructured_client.UnstructuredClient(\n",
    "    api_key_auth=os.getenv(\"UNSTRUCTURED_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e28e0",
   "metadata": {},
   "source": [
    "## Document Processing and Table Extraction\n",
    "\n",
    "A key challenge in RAG systems is extracting structured information from complex formats like PDFs. Here, we'll use Unstructured's API to process a PDF document and extract tables, which are particularly difficult to handle with traditional methods.\n",
    "\n",
    "We'll work with data from the NFL Record & Fact Book to demonstrate how Unstructured preserves table structure for downstream RAG applications.\n",
    "\n",
    "The process involves:\n",
    "1. Downloading the PDF document\n",
    "2. Sending it to Unstructured's API for partitioning\n",
    "3. Extracting tables and their associated titles\n",
    "4. Formatting the extracted data for our RAG pipeline\n",
    "\n",
    "This approach extends to various document types and structures, making it versatile for RAG document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e48e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = \"https://storage.googleapis.com/nfl-2024-record/nfl.pdf\"\n",
    "\n",
    "\n",
    "response = requests.get(file_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "\n",
    "file_content = response.content\n",
    "\n",
    "req = operations.PartitionRequest(\n",
    "    partition_parameters=shared.PartitionParameters(\n",
    "        files=shared.Files(\n",
    "            content=file_content,\n",
    "            file_name=\"nfl.pdf\",\n",
    "        ),\n",
    "        strategy=shared.Strategy.VLM,\n",
    "        vlm_model=shared.PartitionParametersStrategy.GPT_4O,\n",
    "        vlm_model_provider=shared.PartitionParametersSchemasStrategy.OPENAI,\n",
    "        languages=[\"eng\"],\n",
    "        split_pdf_page=True,\n",
    "        split_pdf_allow_failed=True,\n",
    "        split_pdf_concurrency_level=15,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "result = client.general.partition(request=req)\n",
    "\n",
    "\n",
    "if result.elements is None:\n",
    "    raise Exception(\"No elements found in the response\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ca1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawTables = [item for item in result.elements if item[\"type\"] == \"Table\"] # type: ignore\n",
    "titles = [\n",
    "    item\n",
    "    for item in result.elements # type: ignore\n",
    "    if item[\"text\"].startswith(\"TOP\") or \"COACHES\" in item[\"text\"]\n",
    "]\n",
    "\n",
    "\n",
    "tables = [\n",
    "    {\"title\": title[\"text\"], \"table\": table[\"metadata\"][\"text_as_html\"]}\n",
    "    for title, table in zip(titles, rawTables)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43a9fa",
   "metadata": {},
   "source": [
    "## Query Engine Setup\n",
    "\n",
    "Now that we've extracted tables, we need to make this information searchable and retrievable. The query engine will:\n",
    "\n",
    "1. Index the document content (our extracted tables)\n",
    "2. Retrieve relevant information based on user queries\n",
    "3. Provide context to the language model for accurate responses\n",
    "\n",
    "We'll create a query engine using our extracted tables, formatting each with its title for better context. This ensures that when users ask questions about specific statistics, the system retrieves the most relevant tables and provides accurate answers.\n",
    "\n",
    "Our query engine uses the embedding model we set up earlier for semantic search, finding relevant content based on meaning rather than just keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fbf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = tlm_query_engine(\n",
    "    [f\"<h1>{table['title']}</h1>\\n{table['table']}\" for table in tables]\n",
    ")\n",
    "\n",
    "query = input('Enter your query:')\n",
    "print(query_engine.query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e16c6e",
   "metadata": {},
   "source": [
    "## Alternate TLM Method\n",
    "\n",
    "If we prefer using a different LLM while still leveraging Cleanlab's trustworthiness assessment, we can use the TLM's `get_trustworthiness_score` method. This approach allows us to evaluate responses from any LLM without being tied to a specific implementation. Let's set this up using OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c17086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "client.api_key = \"<OPENAI API KEY>\"\n",
    "\n",
    "class LLMWrapper(CustomLLM):\n",
    "    context_window: int = 128_000\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=self.num_output,\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "\n",
    "        trustworthiness = tlm.get_trustworthiness_score(prompt, output or \"\")\n",
    "\n",
    "        return CompletionResponse(\n",
    "            text=json.dumps(\n",
    "                {\n",
    "                    \"response\": output,\n",
    "                    \"trustworthiness_score\": trustworthiness[\"trustworthiness_score\"],  # type: ignore\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=self.num_output,\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        output = \"\"\n",
    "\n",
    "        for token in response:\n",
    "            output += token.choices[0].delta.content or \"\"\n",
    "\n",
    "            yield CompletionResponse(\n",
    "                text=json.dumps(\n",
    "                    {\n",
    "                        \"response\": output,\n",
    "                    }\n",
    "                ),\n",
    "                delta=token.choices[0].delta.content or \"\",\n",
    "            )\n",
    "\n",
    "        trustworthiness = tlm.get_trustworthiness_score(prompt, output)\n",
    "\n",
    "        yield CompletionResponse(\n",
    "            text=json.dumps(\n",
    "                {\n",
    "                    \"response\": output,\n",
    "                    \"trustworthiness_score\": trustworthiness[\"trustworthiness_score\"],  # type: ignore\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def llm_query_engine(documents: list[str]):\n",
    "    return VectorStoreIndex(\n",
    "        [Document(text=document, metadata={\"source\": \"tlm\"}) for document in documents]\n",
    "    ).as_query_engine(llm=LLMWrapper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c03bbc",
   "metadata": {},
   "source": [
    "We can easily run this query engine the same way as the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6adc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_qe = llm_query_engine(\n",
    "    [f\"<h1>{table['title']}</h1>\\n{table['table']}\" for table in tables]\n",
    ")\n",
    "\n",
    "query = input('Enter your query:')\n",
    "print(llm_qe.query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa5ebb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated an integrated approach to building trustworthy document processing systems combining Cleanlab's TLM with Unstructured's document parsing.\n",
    "\n",
    "1. **Enhanced Document Processing**: We've shown how Unstructured transforms complex documents into structured, machine-readable content while preserving semantic relationships and tables. This preprocessing maintains data integrity in RAG systems.\n",
    "\n",
    "2. **Trustworthiness Assessment**: By integrating Cleanlab's TLM, we've implemented a mechanism to quantify response reliability. These scores provide valuable metrics for assessing when to trust model outputs and when human intervention might be necessary.\n",
    "\n",
    "3. **Practical Implementation**: The notebook provides a complete framework that practitioners can adapt to specific domains, from document ingestion to query processing and response evaluation.\n",
    "\n",
    "Integrating trustworthiness metrics into RAG systems significantly advances AI reliability. By providing quantitative confidence measures, these systems enable more informed decision-making about AI-generated content, particularly valuable in high-stakes domains.\n",
    "\n",
    "- [Cleanlab Studio](https://cleanlab.ai/) - Platform for building trustworthy AI systems\n",
    "- [Unstructured Platform](https://unstructured.io/) - Tools for extracting structured information from documents\n",
    "- [Hugging Face Hub](https://huggingface.co/) - Repository of pre-trained models, including embeddings used in this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
