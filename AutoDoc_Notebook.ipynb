{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d01fb0",
   "metadata": {},
   "source": [
    "# Automating Trustworthy Document Processing with Cleanlab and Unstructured\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the rapidly evolving world of artificial intelligence, ensuring the reliability and accuracy of AI-generated responses has become paramount. This notebook demonstrates an approach to building trustworthy document processing systems by combining two powerful technologies: Cleanlab's Trustworthy Language Model (TLM) and Unstructured's document parsing capabilities.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has emerged as a critical methodology for enhancing large language model (LLM) performance by grounding responses in factual, retrieved information. However, the effectiveness of RAG systems is heavily dependent on the quality and accessibility of the underlying data. Many valuable information sources exist in complex formats like PDFs, spreadsheets, and images—formats that require sophisticated preprocessing before they can be utilized in RAG pipelines.\n",
    "\n",
    "This notebook addresses three key challenges in building reliable AI systems:\n",
    "\n",
    "1. **Document Preprocessing**: Converting diverse document formats into structured, machine-readable text while preserving semantic relationships and tabular information.\n",
    "\n",
    "2. **Hallucination Mitigation**: Implementing mechanisms to reduce the generation of false or misleading information by grounding LLM responses in verified data.\n",
    "\n",
    "3. **Trustworthiness Assessment**: Quantifying the reliability of AI-generated responses to enable appropriate human oversight and intervention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27882ee",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages and set up our environment. We'll need several libraries:\n",
    "\n",
    "- `cleanlab-tlm`: For accessing Cleanlab's Trustworthy Language Model\n",
    "- `llama-index`: For building the RAG pipeline\n",
    "- `unstructured_client`: For document processing and extraction\n",
    "- Additional utilities for data handling and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17058ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q cleanlab-tlm llama-index llama-index-embeddings-huggingface unstructured_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c1182",
   "metadata": {},
   "source": [
    "### API Key Configuration\n",
    "\n",
    "To use this notebook, you'll need to obtain API keys for both Cleanlab and Unstructured:\n",
    "- Cleanlab TLM API key: Available at [Cleanlab Studio](https://cleanlab.ai/)\n",
    "- Unstructured API key: Available at [Unstructured.io](https://unstructured.io/)\n",
    "\n",
    "Enter your API keys in the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CLEANLAB_API_KEY\"] = \"\"  # Replace with your API key\n",
    "os.environ[\"UNSTRUCTURED_API_KEY\"] = \"\"  # Replace with your API key\n",
    "os.environ[\"UNSTRUCTURED_API_URL\"] = \"https://api.unstructured.io/general/v0/general\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9254c67",
   "metadata": {},
   "source": [
    "## TLM Integration with LlamaIndex\n",
    "\n",
    "Cleanlab's Trustworthy Language Model (TLM) is designed to provide reliable responses with a trustworthiness score that indicates the model's confidence in its output. This feature is particularly valuable in RAG systems, where ensuring the accuracy of AI-generated responses is critical.\n",
    "\n",
    "In this section, we'll integrate TLM with LlamaIndex, a framework for building RAG applications. This integration involves:\n",
    "\n",
    "1. Initializing a connection to Cleanlab's TLM\n",
    "2. Creating a custom wrapper to use TLM with LlamaIndex\n",
    "3. Setting up an embedding model for semantic search\n",
    "4. Defining a function to create query engines from document collections\n",
    "\n",
    "The trustworthiness scores provided by TLM help identify potential hallucinations or low-confidence responses, enabling more reliable AI applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed8407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "INFO: 2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "from cleanlab_studio import Studio\n",
    "\n",
    "studio = Studio(os.environ[\"CLEANLAB_API_KEY\"])\n",
    "tlm = studio.TLM()\n",
    "\n",
    "import os\n",
    "from typing import Any\n",
    "import json\n",
    "from llama_index.core.base.llms.types import (\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core.llms.custom import CustomLLM\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "\n",
    "\n",
    "studio = Studio(os.environ[\"CLEANLAB_API_KEY\"])\n",
    "tlm = studio.TLM()\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\n",
    "class TLMWrapper(CustomLLM):\n",
    "    context_window: int = 16000\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"TLM\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        # Prompt tlm for a response and trustworthiness score\n",
    "        response = tlm.prompt(prompt)\n",
    "        output = json.dumps(response)\n",
    "        return CompletionResponse(text=output)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        # Prompt tlm for a response and trustworthiness score\n",
    "        response = tlm.prompt(prompt)\n",
    "        output = json.dumps(response)\n",
    "\n",
    "        # Stream the output\n",
    "        output_str = \"\"\n",
    "        for token in output:\n",
    "            output_str += token\n",
    "            yield CompletionResponse(text=output_str, delta=token)\n",
    "\n",
    "\n",
    "def tlm_query_engine(documents: list[str]):\n",
    "    return VectorStoreIndex(\n",
    "        [Document(text=document, metadata={\"source\": \"tlm\"}) for document in documents]\n",
    "    ).as_query_engine(llm=TLMWrapper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f790957",
   "metadata": {},
   "source": [
    "## Unstructured API Setup\n",
    "\n",
    "Unstructured is a powerful document processing platform that can extract structured information from various document formats, including PDFs, Word documents, HTML, and more. It's particularly effective at handling complex layouts, tables, and other structured elements that are challenging to parse with traditional methods.\n",
    "\n",
    "In this section, we'll set up the Unstructured API client to process documents. The Unstructured Platform API provides several key capabilities:\n",
    "\n",
    "1. **Document Partitioning**: Breaking documents into meaningful chunks (paragraphs, tables, lists, etc.)\n",
    "2. **Table Extraction**: Accurately identifying and extracting tabular data\n",
    "3. **Layout Analysis**: Understanding the document's structure and relationships between elements\n",
    "4. **Multi-format Support**: Processing various document formats with a consistent API\n",
    "\n",
    "These capabilities are essential for building effective RAG systems that can leverage information from diverse document sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "472a8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unstructured_client\n",
    "from unstructured_client.models import operations, shared\n",
    "import requests\n",
    "\n",
    "\n",
    "client = unstructured_client.UnstructuredClient(\n",
    "    api_key_auth=os.getenv(\"UNSTRUCTURED_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e28e0",
   "metadata": {},
   "source": [
    "## Document Processing and Table Extraction\n",
    "\n",
    "One of the key challenges in building effective RAG systems is extracting structured information from complex document formats like PDFs. In this section, we'll demonstrate how to use Unstructured's API to process a PDF document and extract tables, which are particularly challenging to handle with traditional text extraction methods.\n",
    "\n",
    "We'll work with data from the NFL Record & Fact Book, which contains various tables with statistics and records. This example illustrates how Unstructured can accurately extract tabular data while preserving its structure, making it suitable for downstream RAG applications.\n",
    "\n",
    "The process involves:\n",
    "1. Downloading the PDF document\n",
    "2. Sending it to Unstructured's API for partitioning\n",
    "3. Extracting tables and their associated titles\n",
    "4. Formatting the extracted data for use in our RAG pipeline\n",
    "\n",
    "This approach can be extended to handle various document types and structures, making it a versatile solution for document processing in RAG systems.\n",
    "\n",
    "Note: Unstructured sometimes fails to partition the PDF. If it does, just rerun the below cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b9e48e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://api.unstructuredapp.io/general/docs \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "file_url = \"https://storage.googleapis.com/richard-xiong-366.appspot.com/nfl.pdf\"\n",
    "\n",
    "\n",
    "response = requests.get(file_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "\n",
    "file_content = response.content\n",
    "\n",
    "req = operations.PartitionRequest(\n",
    "    partition_parameters=shared.PartitionParameters(\n",
    "        files=shared.Files(\n",
    "            content=file_content,\n",
    "            file_name=\"nfl.pdf\",\n",
    "        ),\n",
    "        strategy=shared.Strategy.VLM,\n",
    "        vlm_model=shared.PartitionParametersStrategy.GPT_4O,\n",
    "        vlm_model_provider=shared.PartitionParametersSchemasStrategy.OPENAI,\n",
    "        languages=[\"eng\"],\n",
    "        split_pdf_page=True,\n",
    "        split_pdf_allow_failed=True,\n",
    "        split_pdf_concurrency_level=15,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "result = client.general.partition(request=req)\n",
    "\n",
    "\n",
    "if result.elements is None:\n",
    "    raise Exception(\"No elements found in the response\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ca1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawTables = [item for item in result.elements if item[\"type\"] == \"Table\"] # type: ignore\n",
    "titles = [\n",
    "    item\n",
    "    for item in result.elements # type: ignore\n",
    "    if item[\"text\"].startswith(\"TOP\") or \"COACHES\" in item[\"text\"]\n",
    "]\n",
    "\n",
    "\n",
    "tables = [\n",
    "    {\"title\": title[\"text\"], \"table\": table[\"metadata\"][\"text_as_html\"]}\n",
    "    for title, table in zip(titles, rawTables)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43a9fa",
   "metadata": {},
   "source": [
    "## Query Engine Setup\n",
    "\n",
    "Now that we have extracted tables from our document, we need to make this information searchable and retrievable. This is where the query engine comes into play. The query engine is responsible for:\n",
    "\n",
    "1. Indexing the document content (in this case, our extracted tables)\n",
    "2. Retrieving relevant information based on user queries\n",
    "3. Providing context to the language model for generating accurate responses\n",
    "\n",
    "In this section, we'll create a query engine using the tables we extracted in the previous step. We'll format each table with its title to provide better context for the retrieval system. This approach ensures that when users ask questions about specific records or statistics, the system can retrieve the most relevant tables and provide accurate answers.\n",
    "\n",
    "The query engine leverages the embedding model we set up earlier to perform semantic search, finding the most relevant content based on the meaning of the query rather than just keyword matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fbf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"response\": \"Based on the provided statistics, Derrick Henry stands out as the best rusher to bet on. He has the highest total yards (9,502) and touchdowns (90) among active rushers, with 8 years of experience. His consistent performance and ability to score make him a strong candidate for betting.\", \"trustworthiness_score\": 0.9493786239709094}\n"
     ]
    }
   ],
   "source": [
    "query_engine = tlm_query_engine(\n",
    "    [f\"<h1>{table['title']}</h1>\\n{table['table']}\" for table in tables]\n",
    ")\n",
    "\n",
    "query = input('Enter your query:')\n",
    "print(query_engine.query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e16c6e",
   "metadata": {},
   "source": [
    "## Alternate TLM Method\n",
    "\n",
    "If we want to use a different LLM and simply use Cleanlab's TLM for trustworthiness scoring, we can use the TLM's `get_trustworthiness_score` method to evaluate the trustworthiness of the responses generated by the LLM. This approach allows us to leverage the trustworthiness assessment capabilities of Cleanlab's TLM without being tied to a specific language model implementation. Let's set this up using OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c17086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "client.api_key = \"<OPENAI API KEY>\"\n",
    "\n",
    "class LLMWrapper(CustomLLM):\n",
    "    context_window: int = 128_000\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=self.num_output,\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "\n",
    "        trustworthiness = tlm.get_trustworthiness_score(prompt, output or \"\")\n",
    "\n",
    "        return CompletionResponse(\n",
    "            text=json.dumps(\n",
    "                {\n",
    "                    \"response\": output,\n",
    "                    \"trustworthiness_score\": trustworthiness[\"trustworthiness_score\"],  # type: ignore\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=self.num_output,\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        output = \"\"\n",
    "\n",
    "        for token in response:\n",
    "            output += token.choices[0].delta.content or \"\"\n",
    "\n",
    "            yield CompletionResponse(\n",
    "                text=json.dumps(\n",
    "                    {\n",
    "                        \"response\": output,\n",
    "                    }\n",
    "                ),\n",
    "                delta=token.choices[0].delta.content or \"\",\n",
    "            )\n",
    "\n",
    "        trustworthiness = tlm.get_trustworthiness_score(prompt, output)\n",
    "\n",
    "        yield CompletionResponse(\n",
    "            text=json.dumps(\n",
    "                {\n",
    "                    \"response\": output,\n",
    "                    \"trustworthiness_score\": trustworthiness[\"trustworthiness_score\"],  # type: ignore\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def llm_query_engine(documents: list[str]):\n",
    "    return VectorStoreIndex(\n",
    "        [Document(text=document, metadata={\"source\": \"tlm\"}) for document in documents]\n",
    "    ).as_query_engine(llm=LLMWrapper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c03bbc",
   "metadata": {},
   "source": [
    "We can easily run this query engine the same way as the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6adc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_qe = llm_query_engine(\n",
    "    [f\"<h1>{table['title']}</h1>\\n{table['table']}\" for table in tables]\n",
    ")\n",
    "\n",
    "query = input('Enter your query:')\n",
    "print(llm_qe.query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa5ebb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated an integrated approach to building trustworthy document processing systems by combining Cleanlab's TLM with Unstructured's document parsing capabilities.\n",
    "\n",
    "1. **Enhanced Document Processing Pipeline**: We've shown how Unstructured's API can transform complex document formats like PDFs into structured, machine-readable content while preserving semantic relationships and tabular information. This preprocessing step is crucial for maintaining data integrity in RAG systems.\n",
    "\n",
    "2. **Trustworthiness Assessment Framework**: By integrating Cleanlab's TLM, we've implemented a mechanism to quantify the reliability of AI-generated responses. The trustworthiness scores provide a valuable metric for assessing when to trust model outputs and when human intervention might be necessary.\n",
    "\n",
    "3. **Practical Implementation Strategy**: The notebook provides a complete implementation framework that practitioners can adapt to their specific domains, from document ingestion to query processing and response evaluation.\n",
    "\n",
    "### Implications for AI Reliability\n",
    "\n",
    "The integration of trustworthiness metrics into RAG systems represents a significant advancement in AI reliability. By providing quantitative measures of confidence, these systems enable more informed decision-making about when to trust AI-generated content. This approach is particularly valuable in high-stakes domains where accuracy is critical.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Cleanlab Studio](https://cleanlab.ai/) - Platform for building and deploying trustworthy AI systems\n",
    "- [Unstructured Platform](https://unstructured.io/) - Tools for extracting structured information from unstructured documents\n",
    "- [Hugging Face Hub](https://huggingface.co/) - Repository of pre-trained models, including embedding models used in this notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
